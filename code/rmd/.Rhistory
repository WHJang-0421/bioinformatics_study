table(data$Species)
## now using SMOTE to create a more "balanced problem"
newData <- SMOTE(Species ~ ., data, perc.over = 600,perc.under=100)
table(newData$Species)
## Checking visually the created data
## Not run:
# par(mfrow = c(1, 2))
# plot(data[, 1], data[, 2], pch = 19 + as.integer(data[, 3]),
#      main = "Original Data")
# plot(newData[, 1], newData[, 2], pch = 19 + as.integer(newData[,3]),
#      main = "SMOTE'd Data")
# ## End(Not run)
## Now an example where we obtain a model with the "balanced" data
classTree <- SMOTE(Species ~ ., data, perc.over = 600,perc.under=100,
learner='rpartXse',se=0.5)
## check the resulting classification tree
classTree
## The tree with the unbalanced data set would be
rpartXse(Species ~ .,data,se=0.5)
## A small example with a data set created artificially from the IRIS
## data
data(iris)
data <- iris[, c(1, 2, 5)]
data$Species <- factor(ifelse(data$Species == "setosa","rare","common"))
## checking the class distribution of this artificial data set
table(data$Species)
data
## now using SMOTE to create a more "balanced problem"
newData <- SMOTE(Species ~ ., data, perc.over = 600,perc.under=100)
table(newData$Species)
## Checking visually the created data
## Not run:
# par(mfrow = c(1, 2))
# plot(data[, 1], data[, 2], pch = 19 + as.integer(data[, 3]),
#      main = "Original Data")
# plot(newData[, 1], newData[, 2], pch = 19 + as.integer(newData[,3]),
#      main = "SMOTE'd Data")
# ## End(Not run)
## Now an example where we obtain a model with the "balanced" data
classTree <- SMOTE(Species ~ ., data, perc.over = 600,perc.under=100,
learner='rpartXse',se=0.5)
## check the resulting classification tree
classTree
## The tree with the unbalanced data set would be
rpartXse(Species ~ .,data,se=0.5)
newdata <- SMOTE(
Geo_Location~.,
train,
perc.over = 500,
perc.under = 200
)
newdata <- SMOTE_NC(
Geo_Location~.,
train,
perc.over = 500,
perc.under = 200
)
# divide dependent variables
xtrain <- train[, !names(train) %in% c("Geo_Location")]
data <- read.csv('C:/code_temp/2020_bioinformatics_study/final/data/after_preprocessing/location_mutations.csv')
head(data)
# remove column x
data <- data[, !names(data) %in% c("X")]
head(data)
# change the type of values into logical
mutations <- c('D614G', 'V1228L', 'N501T', 'L5F', 'L18F', 'Q677P', 'Q677H', 'P681H', 'T732A', 'S13I', 'W152C', 'L452R', 'V1176F', 'L54F', 'S477N', 'D138H', 'T859I', 'D253G', 'E780Q', 'A222V', 'G1124V', 'S698L', 'T632N')
for (mutation in mutations){
data[mutation] <- as.logical(unlist(data[mutation]))
}
head(data)
# divide test and train data, stratified by column Geo_Location
set.seed(52)  # for data reproducability
ratio = 0.8 # train ratio
train.index <- createDataPartition(data$Geo_Location, p = ratio, list = FALSE)
train <- data[ train.index,]
test  <- data[-train.index,]
# length of train, test data
length(train$Geo_Location)
length(test$Geo_Location)
sum(train$Geo_Location == "Asia & Europe & Africa")
sum(train$Geo_Location == "North America")
sum(train$Geo_Location == "Oceania")
# divide dependent variables
xtrain <- train[, !names(train) %in% c("Geo_Location")]
ytrain <- train$Geo_Location
xtest <- test[, !names(test) %in% c("Geo_Location")]
ytest <- test$Geo_Location
head(xtrain)
head(ytrain)
write.csv(xtrain, 'C:/code_temp/2020_bioinformatics_study/final/data/after_preprocessing/xtrain.csv')
write.csv(ytrain, 'C:/code_temp/2020_bioinformatics_study/final/data/after_preprocessing/ytrain.csv')
# change the type of values into logical
mutations <- c('D614G', 'V1228L', 'N501T', 'L5F', 'L18F', 'Q677P', 'Q677H', 'P681H', 'T732A', 'S13I', 'W152C', 'L452R', 'V1176F', 'L54F', 'S477N', 'D138H', 'T859I', 'D253G', 'E780Q', 'A222V', 'G1124V', 'S698L', 'T632N')
for (mutation in mutations){
data[mutation] <- as.integer(as.logical(unlist(data[mutation])))
}
head(data)
# divide test and train data, stratified by column Geo_Location
set.seed(52)  # for data reproducability
ratio = 0.8 # train ratio
train.index <- createDataPartition(data$Geo_Location, p = ratio, list = FALSE)
train <- data[ train.index,]
test  <- data[-train.index,]
# length of train, test data
length(train$Geo_Location)
length(test$Geo_Location)
# divide dependent variables
xtrain <- train[, !names(train) %in% c("Geo_Location")]
ytrain <- train$Geo_Location
xtest <- test[, !names(test) %in% c("Geo_Location")]
ytest <- test$Geo_Location
head(xtrain)
head(ytrain)
write.csv(xtrain, 'C:/code_temp/2020_bioinformatics_study/final/data/after_preprocessing/xtrain.csv')
write.csv(ytrain, 'C:/code_temp/2020_bioinformatics_study/final/data/after_preprocessing/ytrain.csv')
install.packages('mlr')
install.packages('imbalance')
library(imbalance)
balanced <- oversample(train, ratio = 0.8, method = "SMOTE")
balanced <- oversample(train, ratio = 0.8, method = "SMOTE", classAttr = "Geo_Location")
balanced
# divide test and train data, stratified by column Geo_Location
set.seed(52)  # for data reproducability
ratio = 0.8 # train ratio
train.index <- createDataPartition(data$Geo_Location, p = ratio, list = FALSE)
train <- data[ train.index,]
test  <- data[-train.index,]
# length of train, test data
length(train$Geo_Location)
length(test$Geo_Location)
# oversampling train data
table(train$Geo_Location)
train.balanced <- oversample(train, ratio = 0.8, method = "SMOTE", classAttr = "Geo_Location")
length(train.balanced$Geo_Location)
# divide test and train data, stratified by column Geo_Location
set.seed(52)  # for data reproducability
ratio = 0.8 # train ratio
train.index <- createDataPartition(data$Geo_Location, p = ratio, list = FALSE)
train <- data[ train.index,]
test  <- data[-train.index,]
# length of train, test data
length(train$Geo_Location)
length(test$Geo_Location)
# oversampling train data
table(train$Geo_Location)
train.balanced <- oversample(train, ratio = 0.8, method = "SMOTE", classAttr = "Geo_Location")
table(train.balanced$Geo_Location)
# divide dependent variables
xtrain <- train.balanced[, !names(train) %in% c("Geo_Location")]
ytrain <- train.balanced$Geo_Location
xtest <- test[, !names(test) %in% c("Geo_Location")]
ytest <- test$Geo_Location
head(xtrain)
head(ytrain)
# divide dependent variables
xtrain <- train.balanced[, !names(train) %in% c("Geo_Location")]
ytrain <- train.balanced$Geo_Location
xtest <- test[, !names(test) %in% c("Geo_Location")]
ytest <- test$Geo_Location
head(xtrain)
1) Training the model
model <- ranger(
formula         = Geo_Location ~ .,
data            = train,
num.trees       = 500,
mtry            = 7,
min.node.size   = 2,
sample.fraction = 0.8,
importance      = 'permutation',
seed            = 71)
print(model)
pred <- predict(model, test)
test_error <- sum(pred$predictions != ytest) / length(ytest) * 100
print(test_error)
model$confusion.matrix
model$variable.importance %>%
tidy() %>%
dplyr::arrange(desc(x)) %>%
dplyr::top_n(10) %>%
ggplot(aes(reorder(names, x), x)) +
geom_col() +
coord_flip() +
ggtitle("Top 10 important variables")
model <- ranger(
formula         = Geo_Location ~ .,
data            = train.balanced,
num.trees       = 500,
mtry            = 7,
min.node.size   = 2,
sample.fraction = 0.8,
importance      = 'permutation',
seed            = 71)
print(model)
model$confusion.matrix
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(ranger)   # a fast c++ implementation of randomforest
library(rpart)
library(caret)
library(dplyr)
library(broom)
library(imbalance)
data <- read.csv('C:/code_temp/2020_bioinformatics_study/final/data/after_preprocessing/location_mutations.csv')
head(data)
# remove column x
data <- data[, !names(data) %in% c("X")]
head(data)
# change the type of values into logical
mutations <- c('D614G', 'V1228L', 'N501T', 'L5F', 'L18F', 'Q677P', 'Q677H', 'P681H', 'T732A', 'S13I', 'W152C', 'L452R', 'V1176F', 'L54F', 'S477N', 'D138H', 'T859I', 'D253G', 'E780Q', 'A222V', 'G1124V', 'S698L', 'T632N')
for (mutation in mutations){
data[mutation] <- as.integer(as.logical(unlist(data[mutation])))
}
head(data)
# divide test and train data, stratified by column Geo_Location
set.seed(52)  # for data reproducability
ratio = 0.8 # train ratio
train.index <- createDataPartition(data$Geo_Location, p = ratio, list = FALSE)
train <- data[ train.index,]
test  <- data[-train.index,]
# length of train, test data
length(train$Geo_Location)
length(test$Geo_Location)
# oversampling train data
table(train$Geo_Location)
train.balanced <- oversample(train, ratio = 0.8, method = "SMOTE", classAttr = "Geo_Location")
table(train.balanced$Geo_Location)
sum(train$Geo_Location == "Asia & Europe & Africa")
sum(train$Geo_Location == "North America")
sum(train$Geo_Location == "Oceania")
# divide dependent variables
xtrain <- train.balanced[, !names(train) %in% c("Geo_Location")]
ytrain <- train.balanced$Geo_Location
xtest <- test[, !names(test) %in% c("Geo_Location")]
ytest <- test$Geo_Location
head(xtrain)
model <- ranger(
formula         = Geo_Location ~ .,
data            = train.balanced,
num.trees       = 500,
mtry            = 7,
min.node.size   = 2,
sample.fraction = 0.8,
importance      = 'permutation',
seed            = 71)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(ranger)   # a fast c++ implementation of randomforest
library(rpart)
library(caret)
library(dplyr)
library(broom)
library(imbalance)
data <- read.csv('C:/code_temp/2020_bioinformatics_study/final/data/after_preprocessing/location_mutation.csv')
head(data)
# remove column x
data <- data[, !names(data) %in% c("X")]
head(data)
# change the type of values into logical
mutations <- c('D614G', 'V1228L', 'N501T', 'L5F', 'L18F', 'Q677P', 'Q677H', 'P681H', 'T732A', 'S13I', 'W152C', 'L452R', 'V1176F', 'L54F', 'S477N', 'D138H', 'T859I', 'D253G', 'E780Q', 'A222V', 'G1124V', 'S698L', 'T632N')
for (mutation in mutations){
data[mutation] <- as.integer(as.logical(unlist(data[mutation])))
}
head(data)
# divide test and train data, stratified by column Geo_Location
set.seed(52)  # for data reproducability
ratio = 0.8 # train ratio
train.index <- createDataPartition(data$Geo_Location, p = ratio, list = FALSE)
train <- data[ train.index,]
test  <- data[-train.index,]
# length of train, test data
length(train$Geo_Location)
length(test$Geo_Location)
# oversampling train data
table(train$Geo_Location)
train.balanced <- oversample(train, ratio = 0.8, method = "SMOTE", classAttr = "Geo_Location")
mutations <- colnames(data).erase("Geo_Location")
mutations.erase("Geo_Location")
# change the type of values into logical
mutations <- colnames(data)
mutations.erase("Geo_Location")
type(mutations)
# change the type of values into logical
mutations <- colnames(data)
typeof(mutations)
for (mutation in mutations){
data[mutation] <- as.integer(as.logical(unlist(data[mutation])))
}
head(data)
# change the type of values into logical
mutations <- colnames(data)
mutation
for (mutation in mutations){
data[mutation] <- as.integer(as.logical(unlist(data[mutation])))
}
head(data)
# change the type of values into logical
mutations <- colnames(data)
typeof(mutations)
for (mutation in mutations){
data[mutation] <- as.integer(as.logical(unlist(data[mutation])))
}
head(data)
# change the type of values into logical
mutations <- colnames(data)
mutations
for (mutation in mutations){
data[mutation] <- as.integer(as.logical(unlist(data[mutation])))
}
head(data)
head(data$Geo_Location)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(ranger)   # a fast c++ implementation of randomforest
library(rpart)
library(caret)
library(dplyr)
library(broom)
library(imbalance)
data <- read.csv('C:/code_temp/2020_bioinformatics_study/final/data/after_preprocessing/location_mutation.csv')
head(data)
# remove column x
data <- data[, !names(data) %in% c("X")]
head(data$Geo_Location)
# change the type of values into logical
mutations <- colnames(data)
mutations
for (mutation in mutations){
data[mutation] <- as.integer(as.logical(unlist(data[mutation])))
}
head(data)
# divide test and train data, stratified by column Geo_Location
set.seed(52)  # for data reproducability
ratio = 0.8 # train ratio
train.index <- createDataPartition(data$Geo_Location, p = ratio, list = FALSE)
# change the type of values into logical
mutations <- colnames(data).erase("Geo_Location")
data <- read.csv('C:/code_temp/2020_bioinformatics_study/final/data/after_preprocessing/location_mutation.csv')
head(data)
# remove column x
data <- data[, !names(data) %in% c("X")]
head(data$Geo_Location)
# remove column x
data <- data[, !names(data) %in% c("X")]
head(data)
# change the type of values into logical
mutations <- colnames(data[, -c("Geo_Location")])
# change the type of values into logical
mutations <- colnames(data[, !names(data) %in% c("Geo_Location")] ])
# change the type of values into logical
mutations <- colnames(data[, !names(data) %in% c("Geo_Location")])
for (mutation in mutations){
data[mutation] <- as.integer(as.logical(unlist(data[mutation])))
}
head(data)
# divide test and train data, stratified by column Geo_Location
set.seed(52)  # for data reproducability
ratio = 0.8 # train ratio
train.index <- createDataPartition(data$Geo_Location, p = ratio, list = FALSE)
train <- data[ train.index,]
test  <- data[-train.index,]
# length of train, test data
length(train$Geo_Location)
length(test$Geo_Location)
# oversampling train data
table(train$Geo_Location)
train.balanced <- oversample(train, ratio = 0.8, method = "SMOTE", classAttr = "Geo_Location")
table(train.balanced$Geo_Location)
train.balanced <- oversample(train.balanced, ratio = 0.8, method = "SMOTE", classAttr = "Geo_Location")
table(train.balanced$Geo_Location)
# divide dependent variables
xtrain <- train.balanced[, !names(train) %in% c("Geo_Location")]
ytrain <- train.balanced$Geo_Location
xtest <- test[, !names(test) %in% c("Geo_Location")]
ytest <- test$Geo_Location
head(xtrain)
model <- ranger(
formula         = Geo_Location ~ .,
data            = train.balanced,
num.trees       = 500,
mtry            = 7,
min.node.size   = 2,
sample.fraction = 0.8,
importance      = 'permutation',
seed            = 71)
model <- ranger(
formula         = Geo_Location ~ .,
data            = train.balanced,
num.trees       = 500,
mtry            = 20,
min.node.size   = 2,
sample.fraction = 0.8,
importance      = 'impurity',
seed            = 71)
print(model)
model$confusion.matrix
model$variable.importance %>%
tidy() %>%
dplyr::arrange(desc(x)) %>%
dplyr::top_n(10) %>%
ggplot(aes(reorder(names, x), x)) +
geom_col() +
coord_flip() +
ggtitle("Top 10 important variables")
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(ranger)   # a fast c++ implementation of randomforest
library(rpart)
library(caret)
library(dplyr)
library(broom)
library(imbalance)
data <- read.csv('C:/code_temp/2020_bioinformatics_study/final/data/after_preprocessing/location_mutation.csv')
head(data)
# remove column x
data <- data[, !names(data) %in% c("X")]
head(data)
# change the type of values into logical
mutations <- colnames(data[, !names(data) %in% c("Geo_Location")])
for (mutation in mutations){
data[mutation] <- as.integer(as.logical(unlist(data[mutation])))
}
head(data)
# divide test and train data, stratified by column Geo_Location
set.seed(52)  # for data reproducability
ratio = 0.8 # train ratio
train.index <- createDataPartition(data$Geo_Location, p = ratio, list = FALSE)
train <- data[ train.index,]
test  <- data[-train.index,]
# length of train, test data
length(train$Geo_Location)
length(test$Geo_Location)
# divide dependent variables
xtrain <- train[, !names(train) %in% c("Geo_Location")]
ytrain <- train$Geo_Location
xtest <- test[, !names(test) %in% c("Geo_Location")]
ytest <- test$Geo_Location
head(xtrain)
model <- ranger(
formula         = Geo_Location ~ .,
data            = train,
num.trees       = 500,
mtry            = 20,
min.node.size   = 2,
sample.fraction = 0.8,
importance      = 'impurity',
seed            = 71)
print(model)
pred <- predict(model, test)
test_error <- sum(pred$predictions != ytest) / length(ytest) * 100
print(test_error)
model$confusion.matrix
model$variable.importance %>%
tidy() %>%
dplyr::arrange(desc(x)) %>%
dplyr::top_n(10) %>%
ggplot(aes(reorder(names, x), x)) +
geom_col() +
coord_flip() +
ggtitle("Top 10 important variables")
tree <- rpart(
Geo_Location ~.,
data = train,
method = "class"
)
print(tree)
tree <- rpart(
Geo_Location ~.,
data = train,
method = "class",
minsplit = 2
)
print(tree)
tree <- rpart(
Geo_Location ~.,
data = train,
method = "class",
mindepth = 3
)
tree <- rpart(
Geo_Location ~.,
data = train,
method = "class"
)
tree <- rpart(
Geo_Location ~.,
data = train,
method = "class"
)
print(tree)
tree <- rpart(
Geo_Location ~.,
data = train,
method = "anova"
)
tree <- rpart(
Geo_Location ~.,
data = train,
method = "anova"
)
print(tree)
fancyRpartPlot(mytree, caption = NULL)
library(rattle)
install.packages('rattle)
install.packages('RColerBrewer')
